<<<<<<< HEAD
---
title: "EE_Downward-Causation_Conjoint"
author: "Johannes Haehnlein"
date: "`r Sys.Date()`"
output: html_document
theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The following workflow analyzes the collected data of a conjoint experiment downward causation in entrepreneurial ecosystems. Downward causation is a key mechanism for understanding how EEs evolve and become self-sustaining by creating feedback loops and reinforcing themselves (Spigel, 2017; Thompson et al., 2018; Wurth et al., 2021). However, despite the importance and potential of downward causation in EEs, research on how to stimulate it remains largely obscured (Mason & Harrison, 2006; Wurth et al., 2021). Based on SET, we propose a conceptual model that examines the likelihood of EE contribution behavior as a potential function of six constructs. We propose that entrepreneurs are more likely to contribute to an EE if they have experienced support from the EE or its actors (H1), if they have strong personal relationships with other actors in the EE (H2), if they feel affiliated with the EE (H3), if they have altruistic motivation (H4), if they expect a benefit from their contribution behavior (H5) and if they perceive a potential benefit for the overall EE (H6). To investigate the proposed hypothesis we adopt a metric conjoint experiment. The data collection followed an attribute-driven and fractional factorial design, in which participants are asked to rate their likelihood of engaging in contribution behavior based on a set of hypothetical profiles that vary in terms of the six introduced attributes, each manipulated at two levels (Aiman-Smith et al., 2002; Shepherd & Zacharakis, 2018). The subsequent workflow uses established measurements and recommendations to analyze conjoint experiments (Schueler et al., 2023; Zhu et al., 2021, Shepherd & Zacharakis, 2018).

### Workflow Steps

### Sampled Data

In our study, we were explicitly interested in factors influencing the decision-making of contribution behavior. Social Exchange Theory was used as a theoretical framework to develop a model, which examines the likelihood of EE contribution behavior as a function of six constructs: reciprocity, personal relationships, affiliation, altruism, rationality, and group gain. In our conjoint experiment. Each attribute can take either a high or low level, resulting in 64 possible profiles. To reduce the number of profiles and avoid multicollinearity, we applied a fractional factorial design reducing the amount of profiles to eight.  The profiles were presented to the participants within a realistic scenario, in which they are approached by a representative of a startup center who wants to get them more involved in the EE in their region by offering them various contribution opportunities such as mentoring, coaching, guest lecturing, or keynote speaking. The participants are asked to rate their likelihood of accepting each profile on a 7-point Likert scale ranging from 1 (very unlikely) to 7 (very likely). The dependent variable is the likelihood of EE contribution behavior. After a first rating round of the eight profiles, the randomized profiles were replicated in a second round. 

We applied a purposive sampling approach focusing on entrepreneurs who have originated from an EE and are anticipated to induce downward causation. Following this framework, we target entrepreneurs who have been supported by public funding programs (e.g., EXIST in Germany) or who have been located in startup centers or participated in incubation or acceleration programs. We collected the data in two waves.

First, we approached the potential participants for our main study through various channels, such as LinkedIn and several supporting institutions from all over Germany, such as universities, incubators, and startup centers. This resulted in 90 responding entrepreneurs. Secondly we used the Prolific online survey platform, assigning the same sampling criteria. This led to additional 100 respondents. 

***

# Libraries

```{r}
library(openxlsx) # Used to import and export excel files
library(tidyverse) # Used to load all tidyverse packages
library(lavaan) # Used to perform confirmatory factor analyses
library(psych) # Used to calculate Cronbach's Alpha and ICC values
library(apaTables) # Used to create correlation tables
library(lme4) # Used for multilevel modeling 
library(lmerTest) # Used for multilevel regression adding p-values to the lme4 package
library(lmtest) # For calculating robust standard errors
library(broom) # Used to create tidy data frames with model results
library(broom.mixed) # Used to tidy result objects for multilevel modeling
library(parameters) # Used to augement regression analyses
library(performance) # Used to  obtain R2 for multilevel models
library(knitr) # Used for table creation
library(clubSandwich) # Used for cluster robust standard errors
library(sandwich) # Used for cluster robust standard errors
library(flextable) # Used for table creation
library(plotly) # To create violin plots
library(viridis) # Provides additional color schemes
```

# Data

## Read Data

```{r}
data <- read.xlsx("Data_EE Downward Causation_Conjoint_Wave2.xlsx")
```

## Inspect Data

### Code Book

* age = Age of the subject
* gender = Gender of the subject
* education = Highest education grade
* busexp = Years of business experience of the entrepreneur
* compfound = Respondent being an entrepreneur
* amountfound = Amount fo founded companies
* compage = Years since (last) startup foundation
* compact = Ongoing activity of the entrepreneur in his startup
* role = Function of the entrepreneur in his startup
* compcity = city of company headquarters
* amountemp = Amount of employees in the entrepreneurs startup
* pubfund1 = public funding by a funding program in germany
* programpart = Participation in an accleration or incubation program
* startupcenter = Being in located in a startup or technology center
* eecontlik = Conjoint Card 1-8 (Assessment of likelihood to engage in the EE contribution behavior)
* eecontlikrep = Replication Conjoint Card 1-8 (Assessment of likelihood to engage in the EE contribution behavior)
* ee_support_reception = Perceived EE support
* ee_affiliation = Affiliation to the EE
* ee_relationships = Personal Relationship to EE actors
* altruism_potential = Altruistic potential
* positive_outcome_expectation = Expected positive outcome of the activity
* benefit_perception = Perceived benefits of the entrepreneur
* evaluation_scenexp = Experience of the described scenario
* evaluation_scenpract = Practicability of the described scenario
* evaluation_studreal = Realism of the described study
* evaluation_critund = Understandability of decision criteria
* evaluation_fun = Fun factor of the study
* ipfinvx = Intense Positive Feeling towards inventing (4 items)
* ipffndx = Intense Positive Feeling towards founding (3 items)
* ipfdev = Intense Positive Feeling towards developing (3 items)
* esex = Entrepreneurial Self-Efficacy (4 items)
* ic_inv = Identity Centrality inventing
* ic_fnd = Identity Centrality founding
* ic_dev = Identity Centrality developing
* ex_orx = Exchange Orientation (9 items)
* selfintx = Self-Interest (3 items)
* othorix = Other Orientation (3 Items)
* gentrustx = General Trust Scale (6 Items)
* bogusx = Bogus Item 1 & 3
* sudoku = Bogus Item 2
* duration = time to complete survey (min)

### Variable names

```{r}
colnames(data)
```

# Data Preparation for Analysis

## Harmonize Variable Names

```{r}
data <- data |>
  rename_all(.funs = tolower) |> # Convert all column names to lower case
  rename("role" = "function") |> # Convert variable "function" to "role"
  rename("icfnd" = "ic_icfnd") |>
  rename("icdev" = "ic_icdev") |>
  rename("icinv" = "ic_icinv") |>
  rename("pubfund" = "pubfund1") |>
  rename("scenexp" = "evaluation_scenexp") |>
  rename("scenpract" = "evaluation_scenpract") |>
  rename("studreal" = "evaluation_studreal") |>
  rename("critund" = "evaluation_critund") |>
  rename("fun" = "evaluation_fun") |>
  rowid_to_column(var = "respondent_id") # Create id column
```

## Remove effortless responses

In order to ensure data quality, respondents with low survey engagement shall be removed from the sample. To check these effortless respondents, we included three bogus items (attention checks) in our survey. We remove all respondents who failed the bogus. Also we remove respondents, who finished the survey too fast or too slow. Based on the survey's pre-test the estimated completion time was between 12 and 20 minutes. According to this, we will remove all respondents, who completed the survey in less than 8 minutes and in more than 40 minutes.

### Remove failed bogus item responses

```{r}
data <- filter(data, bogus1 == 6) # Remove all respondent failing bogus item 1
data <- filter(data, sudoku == 4) # Remove all respondent failing bogus item 2
data <- filter(data, evaluation_bogus3 == 4) # Remove all respondent failing bogus item 2
data <- filter(data, bogus3 == 4) # Remove all respondent failing bogus item 4
```

### Remove too fast or too slow conducting respondents

```{r}
duration_lower <- 8 # response time lower than 8 minutes
duration_higher <- 40 # response time higher than 40 minutes
```

```{r}
time_out_lower <- data |>
  filter(duration > duration_lower) |>
  nrow()
```

```{r}0
time_out_higher <- data |>
  filter(duration < duration_higher) |>
  nrow()
```

```{r}
data <- data |>
  filter(duration >= duration_lower) |>
  filter(duration <= duration_higher)
```

### Sample Size (Wave 1)

```{r}
nrow(data)
```


# Sample demographics and descriptive data

As part of our survey we collected several demographic data of the participating entrepreneurs as well as information about their personal and business background). Subsequently we select the respective data, process them to the correct formats and labels and create visualizations of the data to get an overview and summarizing information of our sample set.   

```{r}
demographics <- select(data, "age", "gender", "education", "busexp", "amountfound", "compage", "compact", "education", "role", "compcity", "amountemp", "pubfund", "programpart", "startupcenter")
```

## Processing and summarizing numerical variables

```{r}

# Calculate means, medians, sds, mins and maxs of numerical variables 

numerical_vars <- demographics |>
  select(age, busexp, amountfound, compage, amountemp) |>
  summarise_all(list(mean = ~round(mean(., na.rm = TRUE), 2), 
                     median = ~round(median(., na.rm = TRUE), 2), 
                     sd = ~round(sd(., na.rm = TRUE), 2), 
                     min = ~round(min(., na.rm = TRUE), 2), 
                     max = ~round(max(., na.rm = TRUE), 2)))


# Reshape the data
numerical_summary_long <- numerical_vars |>
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") |>
  separate(variable, into = c("variable", "statistic"), sep = "_") |>
  pivot_wider(names_from = variable, values_from = value) |>
  relocate(statistic, .before = 1)  

# Creation and Export of flextable

numerical_summary_table <- flextable(numerical_summary_long) |>
    set_header_labels(
    statistic = "Statistic",
    age = "Age",
    busexp = "Business Experience",
    amountfound = "Companies Founded",
    compage = "Company Age",
    amountemp = "Number of Employees") |>
  set_formatter(
    "Age" = ~format(round(., 2), nsmall = 2),
    "Business Experience" = ~format(round(., 2), nsmall = 2),
    "Companies Founded" = ~format(round(., 2), nsmall = 2),
    "Company Age" = ~format(round(., 2), nsmall = 2),
    "Number of Employees" = ~format(round(., 2), nsmall = 2)) |>
  set_table_properties(layout = "autofit") |>
  color(part = "header", color = "white") |>
  bg(part = "header", bg = "gray") |>
  align(align = "center", part = "all")

print(numerical_summary_table)
```

## Processing and summarizing categorical variables

```{r}

# Modify the dataframe to include categorical labels instead of coded numerical lables

demographics <- demographics |>
  mutate(
    gender = case_when(
      gender == 1 ~ "female",
      gender == 2 ~ "male",
      gender == 3 ~ "diverse"),
    education = case_when(
      education == 1 ~ "Lower Secondary Education",
      education == 2 ~ "O-level",
      education == 3 ~ "university of applied sciences entrance qualification",
      education == 4 ~ "higher education entrance qualification",
      education == 5 ~ "completed vocational training",
      education == 6 ~ "university degree",
      education == 7 ~ "Ph.D."),
    role = case_when(
      role == 1 ~ "CEO",
      role == 2 ~ "CTO",
      role == 3 ~ "COO",
      role == 4 ~ "CFO",
      role == 5 ~ "CMO",
      role == 6 ~ "General Manager",
      role == 8 ~ "Director",
      role == 9 ~ "Teamlead",
      role == 10 ~ "Other"),
    compact = if_else(compact == 1, "yes", "no"),
    pubfund = if_else(pubfund == 1, "yes", "no"),
    programpart = if_else(programpart == 1, "yes", "no"),
    startupcenter = if_else(startupcenter == 1, "yes", "no")
    )

# Calculate frequencies and percentages of the variables rounded to two decimals    

categorical_vars <- c("gender", "education", "compact", "role", 
                      "compcity", "pubfund", "programpart", "startupcenter")
categorical_summary_list <- list()

for (var in categorical_vars) {
  cat_var_table <- demographics |>
    count(!!sym(var)) |>
    mutate(percentage = round(n / sum(n) * 100, 2)) |>
    arrange(desc(n))

  categorical_summary_list[[var]] <- cat_var_table
}


# Creation of a flextable

for (var in names(categorical_summary_list)) {
  cat_table <- flextable(categorical_summary_list[[var]]) %>%
    set_table_properties(layout = "autofit") %>%
    color(part = "header", color = "white") %>%
    bg(part = "header", bg = "gray") %>%
    align(align = "center", part = "all") %>%
    autofit()

  print(cat_table)
}

```

## Evaluation of survey feedback

```{r}

# Selection of survey feedback items

survey_feedback <- select(data, "scenexp", "scenpract", "studreal", "critund", "fun")

# Calculate means, medians & sds

survey_feedback_vars <- survey_feedback |>
  select(scenexp, scenpract, studreal, critund, fun) |>
  summarise_all(list(mean = ~round(mean(., na.rm = TRUE), 2), 
                     median = ~round(median(., na.rm = TRUE), 2), 
                     sd = ~round(sd(., na.rm = TRUE), 2)))

# Reshape the data
survey_feedback_long <- survey_feedback_vars %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") |>
  separate(variable, into = c("variable", "statistic"), sep = "_") |>
  pivot_wider(names_from = variable, values_from = value) |>
  relocate(statistic, .before = 1)  

# Creation and Export of flextable

survey_feedback_table <- flextable(survey_feedback_long) |>
    set_header_labels(
    statistic = "Statistic",
    scenexp = "Scenario experience",
    scenpract = "Scenario practicability",
    studreal = "Study realism",
    critund = "Criteria understandability",
    fun = "Perceived fun") |>
  set_formatter(
    "Scenario experience" = ~format(round(., 2), nsmall = 2),
    "Scenario practicability" = ~format(round(., 2), nsmall = 2),
    "Study realism" = ~format(round(., 2), nsmall = 2),
    "Criteria understandability" = ~format(round(., 2), nsmall = 2),
    "Perceived fun" = ~format(round(., 2), nsmall = 2)) |>
  set_table_properties(layout = "autofit") |>
  color(part = "header", color = "white") |>
  bg(part = "header", bg = "gray") |>
  align(align = "center", part = "all")

print(survey_feedback_table)

```

## Removal of data not being used further in the analysis 

```{r}
data <- data |>
  select(-c(scenexp)) |>
  select(-c(scenpract)) |>
  select(-c(studreal)) |>
  select(-c(critund)) |>
  select(-c(fun)) |>
  select(-c(education)) |>
  select(-c(role)) |>
  select(-c(compact)) |>
  select(-c(busexp)) |>
  select(-c(amountfound)) |>
  select(-c(compfound)) |>
  select(-c(compage)) |>
  select(-c(compcity)) |>
  select(-c(bogus1)) |>
  select(-c(sudoku)) |>
  select(-c(evaluation_bogus3)) |>
  select(-c(bogus3)) |>
  select(-c(amountemp)) |>
  select(-c(duration)) 

```
















# Analysis of measured variables

To control the findings of our conjoint experiment with different potential influencing factors, we included the earlier described demographic information as well as different constructs, that theoretically could have a moderating influence. These latent variables are:

- General Trust Scale explained by 6 items (Yamagishi & Yamagishi, 1994)
- Self-Interest explained by 3 items (De Dreu & Nauta, 2009)
- Other Orientation explained by 3 items (De Dreu & Nauta, 2009)
- Exchange Orientation explained by 9 items (Clark & Mills, 2012)
- Entrepreneurial Self-Efficacy explained by 4 items (Zhao et al., 2005)
- Entrepreneurial Passion by passion for inventing (explained by 4 items for intense positive feelings towards inventing and 1 item for identity centrality for inventing), passion for founding (explained by 3 items for intense positive feelings towards founding and 1 item for identity centrality for founding) and passion for developing (explained by 3 items for intense positive feelings towards developing and 1 item for identity centrality for developing) (Cardon et al. 2013)

## Confirmatory factor analysis

Subsequently we conduct a confirmatory factor analysis (CFA) to evaluate the factor loadings on the latent vaiables and to test the model fit.

### Description of CFA model


```{r}

# Score reversion of exchange orientation items as indicated by the original scales (Clark & Mills, 2012)

data <- data |>
  mutate(  exor3 = 8 - exor3,
    exor4 = 8 - exor4,
    exor5 = 8 - exor5,
    exor9 = 8 - exor9
  ) 

# CFA model

cfa_model <- "gen_trust =~ gentrust1 + gentrust2 + gentrust1 + gentrust4 + gentrust5 + gentrust6
              self_int =~ selfint1 + selfint2 + selfint3
              oth_ori =~ othori1 + othori2 + othori3
              ex_or =~ exor1 + exor2 + exor3 + exor4 + exor5 + exor6 + exor7 + exor8 + exor9
              ese =~ ese1 + ese2 + ese3 + ese4
              ipf_inv =~ ipfinv1 + ipfinv2 + ipfinv3 + ipfinv4
              ic_inv =~ icinv
              ipf_fnd =~ ipffnd1 + ipffnd2 + ipffnd3 + ipffnd1
              ic_fnd =~ icfnd
              ipf_dev =~ ipfdev1 + ipfdev2 + ipfdev3
              ic_dev =~ icdev
              ep_inv =~ ipf_inv + ic_inv
              ep_fnd =~ ipf_fnd + ic_fnd
              ep_dev =~ ipf_dev +   ic_dev
              ep =~ ep_inv + ep_fnd + ep_dev"
                
```



### Model fit information

```{r}

cfa_fit <- cfa(cfa_model, data = data)

summary(cfa_fit, fit.measures = TRUE)

```

### Standardization of results

```{r}

cfa_fit <- standardizedsolution(cfa_fit)

summary(cfa_fit, fit.measures = TRUE)

```

### CFA: Results Overview in a table

```{r}

cfa_fit <- cfa_fit |>
  filter(op == "=~") |>
  select(latent_var = lhs, indicators = rhs, std_loading = est.std ) |>
  mutate(std_loading = round(std_loading, 2))

cfa_fit_table <- flextable(cfa_fit) |>
  autofit() |>
  color(part = "header", color = "white") |>
  bg(part = "header", bg = "gray") |>
  align(align = "center", part = "all") |>
  fontsize(size = 12, part = "all")

print(cfa_fit_table)

```

## Aggregation of indicators to composites

### Calculating the composites

```{r}
data <- data |>
  rowwise() |>
  mutate(gen_trust = mean(c(gentrust1, gentrust2, gentrust3, gentrust4, gentrust5, gentrust6))) |>
  mutate(self_int = mean(c(selfint1, selfint2, selfint3))) |>
  mutate(oth_ori = mean(c(othori1, othori2, othori3))) |>
  mutate(ex_or = mean(c(exor1, exor2, exor3, exor4, exor5, exor6, exor7, exor8, exor9))) |>
  mutate(ese = mean(c(ese1, ese2, ese3, ese4))) |>
  mutate(ipf_inv = mean(c(ipfinv1, ipfinv2, ipfinv3, ipfinv4))) |>
  mutate(ic_inv = mean(c(icinv))) |>
  mutate(ipf_dev = mean(c(ipfdev1, ipfdev2, ipfdev3))) |>
  mutate(ic_dev = mean(c(icdev))) |>
  mutate(ipf_fnd = mean(c(ipffnd1, ipffnd2, ipffnd3))) |>
  mutate(ic_fnd = mean(c(icfnd))) |>
  mutate(ep_inv = mean(c(ipf_inv, ic_inv))) |>
  mutate(ep_fnd = mean(c(ipf_fnd, ic_fnd))) |>
  mutate(ep_dev = mean(c(ipf_dev, ic_dev))) |>
  mutate(ep = mean(c(ep_inv, ep_fnd, ep_dev))) |>
  ungroup()
  
```

### Removing the indicators from the dataset

```{r}

data <- data |>
  select(-c(gentrust1, gentrust2, gentrust3, gentrust4, gentrust5, gentrust6)) |>
  select(-c(selfint1, selfint2, selfint3)) |>
  select(-c(othori1, othori2, othori3)) |>
  select(-c(exor1, exor2, exor3, exor4, exor5, exor6, exor7, exor8, exor9)) |>
  select(-c(ese1, ese2, ese3, ese4)) |>
  select(-c(ipfinv1, ipfinv2, ipfinv3, ipfinv4)) |>
  select(-c(icinv)) |>
  select(-c(ipfdev1, ipfdev2, ipfdev3)) |>
  select(-c(icdev)) |>
  select(-c(ipffnd1, ipffnd2, ipffnd3)) |>
  select(-c(icfnd)) 

```













# Conjoint experiment

## Fractional Factorial Design 

Here, we provide you with the factorial design of this conjoint study: Subsequently the factorial design of the conducted conjoint study will be explained: 

  * 6 attributes
    **ee_support_reception = Perceived EE support
    **ee_affiliation = Affiliation to the EE
    **ee_relationships = Personal Relationship to EE actors
    **altruism_potential = Altruistic potential
    **positive_outcome_expectation = Expected positive outcome of the activity
    **benefit_perception = Perceived benefits of the entrepreneur
  * 2 levels per attribute (0 = low; 1 = high)
  * Fractional design with main-effects only  -> 8 profiles
  * Full replication of all 8 profiles -> 16 profiles
  * Dependent variable = eecontlikex = likelihood to engage in the EE contribution behavior

```{r}
# Levels of all 8 profiles (Orthoplan)

profiles <- tibble(
  profile = c(1, 2, 3, 4, 5, 6, 7, 8),
  ee_support_reception = c(1, 0, 1, 0, 0, 1, 0, 1),
  ee_affiliation = c(0, 1, 1, 0, 1, 0, 0, 1),
  ee_relationships = c(0, 1, 0, 0, 0, 1, 1, 1),
  altruism_potential = c(0, 0, 1, 1, 0, 0, 1, 1),
  positive_outcome_expectation = c(0, 0, 0, 1, 1, 1, 0, 1),
  benefit_perception = c(1, 1, 0, 1, 0, 0, 0, 1)
)
```

### First round of data collection in the conjoint experiment

  *Included variables:
    **respondent_id
    **age
    **gender
    **pubfund
    **programpart
    **startupcenter
    **eecontlike1 to eecontlike8
    **[CFA Factors]


```{r}

first_round_data <- data |>
  select(1:14) # Selecting data from first collection round

first_round_data <- first_round_data |>
  pivot_longer(!c(1:6), # pivoting the data set to a long format
    names_to = "profile", # bringing profile numbers to profile column
    values_to = "eecontlike") |> # bringing eecontlike values to eecontlike column
  mutate(profile = as.numeric(str_extract(profile, "\\-*\\d+\\.*\\d*"))) |> # saving numeric values of profile numbers into the profile column
  mutate(round_id = 1) |> # adding round 1 id
  relocate(round_id, profile, eecontlike, .after = respondent_id) # adjusting column order

```

### Second round of data collection in the conjoint experiment

  *Included variables:
    **respondent_id
    **age
    **gender
    **pubfund
    **programpart
    **startupcenter
    **eecontlike1rep to eecontlike8rep
    **[CFA Factors]


```{r}

second_round_data <- data |>
  select(1:6, 15:22) # Selecting data from second collection round

second_round_data <- second_round_data |>
  pivot_longer(!c(1:6), # pivoting the data set to a long format
    names_to = "profile", # bringing profile numbers to profile column
    values_to = "eecontlike") |> # bringing eecontlike values to eecontlike column
  mutate(profile = as.numeric(str_extract(profile, "\\-*\\d+\\.*\\d*"))) |> # saving numeric values of profile numbers into the profile column
  mutate(round_id = 2) |> # adding round 1 id
  relocate(round_id, profile, eecontlike, .after = respondent_id) # adjusting column order

```

### Merging both reounds of data collection

```{r}
data <- rbind(first_round_data, second_round_data)
```

### Merging factorial design

```{r}
data <- left_join(data, profiles, by = "profile") |> 
  relocate("ee_support_reception", "ee_affiliation", "ee_relationships", "altruism_potential", "positive_outcome_expectation", "benefit_perception", .after = "profile") #relocating attributes in the data set

```

### Converting id and round variables from a numeric format into factors

```{r}

data <- data |>
   mutate(respondent_id = as_factor(respondent_id)) |>
   mutate(round_id = as_factor(round_id))

```


## Evaluation of data consistency

Due to the nature of conjoint studies with complex and theoretically derived scenarios, the potential uncertainity, ambiguity and eventually careless responses is high (Lohrke, 2010). In order to test response consistency, we replicated all 8 profiles of our fractional factorial design (Shepherd & Zacharakis, 2018). Using the data sets from both data collection rounds, we can test the stability of responses applying test-retest reliability assesment.

To evaluate test-retest reliability, we follow the subsequent workflow proposed by Schueler et al. (2023):

1. Calculation of the Intraclass Correlation Coefficient (ICC) for each decision profile
2. Estimation of a linear model with clustered standard errors
3. Calculation of a slope difference test for each parameter
4. Specification of a fully pooled model using both rounds of data with two-way clustered standard errors for both, round and respondent

Before implementing this workflow for the

### Help Functions

#### Help Functions Pearson's R

This function takes a data set as an input, splits it into the initial and replication responses, calculates the correlation for each profile, and returns a correlation vector.


```{r}
rel_cor <- function(data) {

# Selecting conjoint profile data from first collection round  
    first_round_data_rel <- data |>
    filter(round_id == 1) |>
    select(respondent_id, profile, eecontlike) |>
    group_by(profile) |>
    group_split()   

# Selecting conjoint profile data from second collection round (replication)   
  second_round_data_rel <- data |>
    filter(round_id == 2) |>
    select(respondent_id, profile, eecontlike) |>
    group_by(profile) |>
    group_split()   

# Calculating correlation for each profile
  cor <- map2(first_round_data_cor, second_round_data_cor, ~ cor(.x$eecontlike, .y$eecontlike))   

# Collapsing list to vector
  cor <- unlist(cor)   

# Returning correlation vector
  return(cor)   
}
```

#### Help Functions ICC(3,k)

This function takes a data set as an input, splits it into the initial and replication responses, calculates the ICC(3,k) for each profile by calling an additional helper function, and returns a data frame with ICC(3,k) and its 95% confidence interval.

```{r}
rel_icc <- function(data) {

# Selecting conjoint profile data from first collection round
  first_round_data_rel <- data |>
    filter(round_id == 1) |>
    select(respondent_id, profile, eecontlike) |>
    group_by(profile) |>
    group_split() 

# Selecting conjoint profile data from second collection round (replication) 
  second_round_data_rel <- data |>
    filter(round_id == 2) |>
    select(respondent_id, profile, eecontlike) |>
    group_by(profile) |>
    group_split() 

# Map over all profiles and compute the ICC(3,k)
  res <- map2_dfr(first_round_data_rel, second_round_data_rel, ~ icc_3k(.x, .y)) # Calculating the ICC(3,k) for all profiles

# Returning correlation vector    
  return(res) 
}

# function to compute the ICC(3,k)
  icc_3k <- function(first_round_data_rel, second_round_data_rel) {
  
# Getting all profiles 
  profile <- unique(first_round_data_rel$profile) 

# Creating a temporary data set
  tmp_data <- tibble(first_round_data_rel = first_round_data_rel$eecontlike, second_round_data_rel = second_round_data_rel$eecontlike) 

# Computing ICCs and extracting ICC(3,k) and its 95% confidence interval
  icc <- ICC(tmp_data, missing = TRUE, 
             alpha = .05, 
             lmer = FALSE, 
             check.keys = FALSE)$results |>
    filter(type == "ICC3k") |>
    select(ICC, "lower bound", "upper bound") |>
    rename(icc_lower = "lower bound", icc_upper = "upper bound") |>
    mutate_all(round, 2) |>
    mutate(ICC = if_else(ICC < 0, 0, ICC)) |>
    mutate(icc_lower = if_else(icc_lower < 0, 0, icc_lower)) |> 
    remove_rownames(.)   

# Create table
  res <- cbind(profile, icc)

# Return result
  return(res)
}

```

#### Help functions Slope Difference Tests

This function takes a data set as an input, splits it into initial and replication responses, fits two linear models with robust standard errors and then calls an additional function to perform a slope difference test an each coefficient. This additional function takes a variable name, two beta coefficients and standard errors as inputs to conduct perform a slope difference test.

```{r}
slope_difference <- function(data) {
  
# Selecting conjoint profile data from first collection round
  first_round_data_sdt <- data |> 
    filter(round_id == 1) 

# Selecting conjoint profile data from second collection round (replication)
  second_round_data_sdt <- data |> 
    filter(round_id == 2) 
  
# Creating regression formula
  reg_form <- first_round_data_sdt |>
    select(-c(respondent_id, round_id, profile, eecontlike)) |>
    colnames() |>
    paste0(collapse = " + ")
  reg_form <- formula(paste0("eecontlike ~ ", reg_form))

# Fitting a linear regression model on both rounds of responses
  m1.model <- lm(reg_form, data = first_round_data_sdt)
  m2.model <- lm(reg_form, data = second_round_data_sdt)

# Calculating and capturing clustered standard errors
  m1.tidy <- model_parameters(
    m1.model,
    vcov = "vcovCL",
    vcov_args = list(type = "HC1", cluster = first_round_data_sdt$respondent_id)
  )

  m2.tidy <- model_parameters(
    m2.model,
    vcov = "vcovCL",
    vcov_args = list(type = "HC1", cluster = second_round_data_sdt$respondent_id)
  )

# Dropping intercept to tidy the dataframe
  m1.tidy <- filter(m1.tidy, Parameter != "(Intercept)")
  m2.tidy <- filter(m2.tidy, Parameter != "(Intercept)")

# Transforming data frame rows into a list of single row data frames
  m1.tidy <- m1.tidy |>
    group_by(Parameter) |>
    group_split()

  m2.tidy <- m2.tidy |>
    group_by(Parameter) |>
    group_split()

# Applying the slope difference test function to the lists of single row data frames
  res <- map2_dfr(m1.tidy, m2.tidy, 
                  ~ get_difference(name = .x$Parameter,
                                   b1 = .x$Coefficient,
                                   se1 = .x$SE,
                                   b2 = .y$Coefficient,
                                   se2 = .y$SE))

# Return result
  return(res)
}

get_difference <- function(name, b1, se1, b2, se2) {
  
# Computing the difference between both beta coefficients
  b <- b1 - b2

# Computing the corresponding standard error
  se <- sqrt((se1^2) + (se2^2))

# Multiplying the standard error by two to obtain at least the 95% CI
  test <- se * 2

# Checking if the absolute beta difference is at least two SEs away from zero
  if (abs(b) >= test) {
    
# If true, then both slopes are significantly different from another
    res <- "Yes"
    
  } else {
    
# If false, both slopes are not significantly different from another
    res <- "No"
    
  }

# Creating table
  res <- tibble(Coefficient = name, 
                Beta_diff = b, 
                Joint_se = se, 
                Test_statistic = abs(b) - (2 * se), 
                Stat_diff = res)

# Rounding all values to two digits
  res <- mutate(res, across(where(is.double), round, 2))

# Returning result
  return(res)
}
```

#### Help functions Pooled Regression

This function takes a data set as an input and fits a pooled regression model.

```{r}
pooled_regression <- function(data) {
  
# Converting the grouping variables to a factor
  multi.df <- data |>
    mutate(round_id = as_factor(round_id),
           respondent_id = as_factor(respondent_id))

# Creating regression formula
  reg_form <- multi.df |>
    select(-c(respondent_id, round_id, profile, eecontlike)) |>
    colnames() |>
    paste0(collapse = " + ")
  reg_form <- formula(paste0("dv ~ ", reg_form))

# Fitting a linear regression model
  m3_model <- lm(reg_form, data = multi.df)

# Obtaining and showing two-way clustered standard errors (by respondent and round)
  res <- coeftest(m3_model, vcov = vcovCL, cluster = ~ respondent_id + round_id)

# Tidying result
  param <- tidy(res)

# Changing labels
  param <- param |>
    mutate(estimate = round(estimate, 2)) |>
    mutate(std.error = round(std.error, 2)) |>
    mutate(statistic = round(statistic, 2)) |>
    mutate(p.value = round(p.value, 4)) |>
    rename(Coefficient = term) |>
    rename(B = estimate) |>
    rename(`t-ratio` = statistic) |>
    rename(`p-value` = p.value) |>
    rename(SE = std.error)

# Extracting model fit
  fit_pr <- glance(m3_model)

# Changing labels
  fit_pr <- tibble(
    Coefficient = c(colnames(fit)[[1]], 
                    colnames(fit)[[2]], 
                    colnames(fit)[[3]], 
                    "Decisions", 
                    "Respondents"),
    B = c(round(fit$r.squared, 2), 
          round(fit$adj.r.squared, 2), 
          round(fit$sigma, 2), 
          fit$nobs, 
          length(unique(multi.df$respondent_id))),
    SE = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_),
    `t-ratio` = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_),
    `p-value` = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_))

# Adding fit to results
  res <- rbind(param, fit_pr)

# Returning result
  return(res)
}
```

#### Violin Plots

##### Response Deviations

This function takes a data set as an input, splits it into initial and replication responses and computes, for each profile, the difference between responses for each respondent.

```{r}
compute_deviation <- function(data) {
  
# Selecting conjoint profile data from first collection round
  first_round_data_rd <- data |> 
    filter(round_id == 1) |> 
    select(respondent_id, profile, eecontlike) |>
    group_by(profile) |> 
    group_split()
  
# Selecting conjoint profile data from second collection round (replication)
  second_round_data_rd <- data |> 
    filter(round_id == 2) |> 
    select(respondent_id, profile, eecontlike) |> 
    group_by(profile) |> 
    group_split()
  
# Extracting respondent id
  respondent_id <- map(first_round_data_rd, ~ select(.x, respondent_id))
  respondent_id <- map(id, ~ unlist(.x$respondent_id))
  respondent_id <- tibble(respondent_id = respondent_id[[1]])
  
# Selecting the eecontlike column (First round of data collection)
  first_round_data_tmp <- map(first_round_data_rd, ~ select(.x, eecontlike))
  
# Turning data frame into a numeric vector (First round of data collection)
  first_round_data_tmp <- map(first_round_data, ~ unlist(.x$eecontlike))
  
# Select the eecontlike column (Second round of data collection)
  second_round_data_tmp <- map(second_round_data_tmp, ~ select(.x, eecontlike))
  
# Turning data frame into numeric vector (Second round of data collection)
  second_round_data_tmp <- map(second_round_data_tmp, ~ unlist(.x$eecontlike))
  
# Subtracting initial response from replication response
  dev_dat <- map2(second_round_data_tmp, first_round_data_tmp, ~ .x - .y)
  
# Converting list to data frame
  dev_dat <- as_tibble(do.call("cbind", dev_dat))
  dev_dat <- cbind(respondent_id, dev_dat)
  
# Returning result
  return(dev_dat)
}
```

##### Data Wide to Long

This function takes a data set as an input and pivots it from wide to long, so that there is an ID column, a profile column, and a deviation column. This format is required to create a violin plot.

```{r}
wide_to_long <- function(data) {
  
# Pivoting the data frame from wide to long
  data <- data |>
    pivot_longer(!respondent_id, names_to = "profile", values_to = "deviation") |>
    mutate(profile = paste0("Profile ", str_extract(profile, "\\d+")))

# Returning result
  return(data)
}
```

##### Create Violin Plot

This function requires a data set, the number of profiles, the width and the height of the plot, and its name, as inputs and creates a violin plot.


```{r}
violin_plot <- function(data, num_profiles = 8, plot_height = 15, plot_width = 20, plot_name = violin_plot_eecontlike) {
  
# Profile names as factor with ordered levels to preserve order
  data$profile <- factor(data$profile, levels = unique(data$profile))

# Custom text styling for axis labels
  t1 <- list(
    family = "Times New Roman",
    color = "black",
    face = "bold",
    size = 12
  )

# Custom text styling for axis labels
  t2 <- list(
    family = "Times New Roman",
    color = "black",
    size = 16
  )

# Creating violin plot with plotly
  p <- plot_ly(data,
               width = plot_width, 
               height = plot_height,
               y = ~deviation, 
               color = ~profile,
               type = "violin", 
               colors = viridis_pal(option = "C")(num_profiles),
               box = list(visible = T, line = list(color = "dimgrey")), 
               meanline = list(visible = T, color = "black")) |>
    layout(font = t1) %>%
    layout(paper_bgcolor = "white") |>
    layout(plot_bgcolor = "transparent") |>
    layout(yaxis = list(
      title = list(text = "<b>Deviation Magnitude Distribution</b>", font = t2),
      gridcolor = "lightgrey", 
      zerolinecolor = "grey")) |>
    layout(xaxis = list(
      categoryarray = ~profile, 
      categoryorder = "array")) |>
    layout(xaxis = list(
      title = list(text = "<b>Profile Level</b>", font = t2),
      gridcolor = "#ffff", zerolinecolor = "#ffff")) |>
    layout(showlegend = FALSE) |>
    config(displaylogo = FALSE, modeBarButtonsToRemove = c(
      "toggleSpikelines",
      "hoverClosestCartesian",
      "hoverCompareCartesian")) |>
    config(toImageButtonOptions = list(
      format = "png",
      filename = plot_name,
      scale = 1))

  # Return result
  return(p)
}
```

### ICC Effect Plot

This function takes a data set as an input and creates an ICC effect plot.

```{r}
icc_effect_plot <- function(data) {
  
  # Create the plot
  plot <- ggplot(data = data, 
                 aes(x = profile, 
                     y = ICC, 
                     ymin = icc_lower, 
                     ymax = icc_upper)) +
    geom_pointrange() +
    geom_text(aes(label = ICC), nudge_x = 0.28) +
    labs(
      title = paste0("ICC Values -- Profiles 1-", length(data$profile)),
      subtitle = "95% Confidence Interval As Vertical Line",
      x = NULL,
      y = "Intraclass Correlation Coefficients") +
    theme_minimal()

  # Return result
  return(plot)
}
```


### Slope Difference Effect Plot

This function takes a data set as an input and creates slope difference effect plot.

```{r}
slope_effect_plot <- function(data) {
  
  # Create the plot
  plot <- ggplot(data = data, aes(x = Coefficient, y = Test_statistic)) +
    geom_point() +
    geom_text(aes(label = Test_statistic), nudge_x = 0.25) +
    coord_flip() +
    geom_hline(yintercept = 0, size = 2) +
    annotate("text",
      x = 2.5, y = 0.01, angle = 90,
      label = "Statistically Significant at Zero or Higher") +
    labs(
      title = "Slope Difference Test Results",
      subtitle = "No Statistically Significant Difference for Test Statistic Below Zero",
      y = "Test Statistic",
      x = "Independent Variables") +
    theme_minimal()

  # Return plot
  return(plot)
}
```

### Replication Reliability Comparisons

This function takes two data sets as inputs and requires information how many profiles reflect a replication of 25% of the profiles etc. With this input, the function computes the mean ICC and Pearson's for each replication variant.

```{r}
rel_rep <- function(dat_icc = data, dat_cor = data, rep_25 = 2, rep_50 = 4, rep_75 = 6, rep_100 = 8) {
  
# Computing the mean ICC for a replication of 25% of the profiles
  icc_25 <- dat_icc |>
    filter(profile == c(1:rep_25)) |>
    summarise(icc = round(mean(ICC), 2), 
              icc_lower = round(mean(icc_lower), 2), 
              icc_upper = round(mean(icc_upper), 2))
  
# Computing the mean ICC for a replication of 50% of the profiles
  icc_50 <- dat_icc %>%
    filter(profile == c(1:rep_50)) |>
    summarise(icc = round(mean(ICC), 2), 
              icc_lower = round(mean(icc_lower), 2), 
              icc_upper = round(mean(icc_upper), 2))

# Computing the mean ICC for a replication of 75% of the profiles
  icc_75 <- dat_icc |>
    filter(profile == c(1:rep_75)) |>
    summarise(icc = round(mean(ICC), 2), 
              icc_lower = round(mean(icc_lower), 2), 
              icc_upper = round(mean(icc_upper), 2))

# Computing the mean ICC for a replication of 100% of the profiles
  icc_100 <- dat_icc |>
    filter(profile == c(1:rep_100)) |>
    summarise(icc = round(mean(ICC), 2), 
              icc_lower = round(mean(icc_lower), 2), 
              icc_upper = round(mean(icc_upper), 2))

# Computing the mean correlation for a replication of 25% of the profiles
  cor_25 <- dat_cor |>
    filter(profile == c(1:rep_25)) |>
    summarise(cor = round(mean(r), 2))

# Computing the mean correlation for a replication of 50% of the profiles
  cor_50 <- dat_cor %>%
    filter(profile == c(1:rep_50)) |>
    summarise(cor = round(mean(r), 2))
  
# Computing the mean correlation for a replication of 75% of the profiles
  cor_75 <- dat_cor |>
    filter(profile == c(1:rep_75)) |>
    summarise(cor = round(mean(r), 2))

# Computing the mean correlation for a replication of 100% of the profiles
  cor_100 <- dat_cor |>
    filter(profile == c(1:rep_100)) |>
    summarise(cor = round(mean(r), 2))

# Combining ICCs and correlations
  reliability_25 <- cbind(cor_25, icc_25)
  reliability_50 <- cbind(cor_50, icc_50)
  reliability_75 <- cbind(cor_75, icc_75)
  reliability_100 <- cbind(cor_100, icc_100)
  replications <- tibble(replications = c("25%", "50%", "75%", "100%"))

# Combining everything into a single table
  reliability <- rbind(reliability_25, 
                       reliability_50, 
                       reliability_75, 
                       reliability_100)

# Merging replication information with result table
  replications <- cbind(replications, reliability)

# Returning result
  return(replications)
}
```


## Step 1: Reliabilities

### Pearson's r

```{r}
# Get profile-level correlations
data_cor <- map(data, ~ rel_cor(data = .x))

# Create table
data_cor[[1]] <- tibble(profile = c(1,2,3,4,5,6,7,8), r = round(data_cor[[1]], 2))

```


### ICC(3,k)

```{r}
# Get profile-level ICCs
data_icc <- map(data, ~ rel_icc(data =  = .x))
```


### Combine Reliabilities

```{r}
# Combine both reliabilities for all three data sets
data_rel <- map2(data_cor, data_icc, ~ left_join(.x, .y, by = "profile"))


# Create mean row to be placed at the bottom
means <- tibble(profile = "Mean", 
                "r.x" = mean(data_rel$r.x, na.rm = TRUE),
                "ICC.x" = mean(data_rel$ICC.x, na.rm = TRUE),
                "icc_lower.x" = mean(data_rel$icc_lower.x, na.rm = TRUE),
                "icc_upper.x" = mean(data_rel$icc_upper.x, na.rm = TRUE),
                "r.y" = mean(data_rel$r.y, na.rm = TRUE),
                "ICC.y" = mean(data_rel$ICC.y, na.rm = TRUE),
                "icc_lower.y" = mean(data_rel$icc_lower.y, na.rm = TRUE),
                "icc_upper.y" = mean(data_rel$icc_upper.y, na.rm = TRUE),
                "r" = mean(data_rel$r, na.rm = TRUE),
                "ICC" = mean(data_rel$ICC, na.rm = TRUE),
                "icc_lower" = mean(data_rel$icc_lower, na.rm = TRUE),
                "icc_upper" = mean(data_rel$icc_upper, na.rm = TRUE))

# Round means to two decimals
means <- mutate(means, across(where(is.double), round, 2))

# Add mean row at the bottom of the data frame
data_rel <- rbind(data_rel, means)
```


### Create Table

```{r}
# Use flextable to create a nice table for publication
data_rel |>
  flextable() |>
  align(align = "center", j = c(1:13), part = "header") |>
  align(align = "center", j = c(1:13), part = "body") |>
  add_header_row(
    colwidths = c(1, 4, 4, 4),
    values = c(
      "",
      "3 Attributes 2 Levels",
      "3 Attributes 3 Levels",
      "5 Attributes 2 Levels")) |>
  set_header_labels(
    profile = "Profile",
    r.x = "r",
    ICC.x = "ICC3k",
    icc_lower.x = "ICC3k lower",
    icc_upper.x = "ICC3k upper",
    r.y = "r",
    ICC.y = "ICC3k",
    icc_lower.y = "ICC3k lower",
    icc_upper.y = "ICC3k upper",
    r = "r",
    ICC = "ICC3k",
    icc_lower = "ICC3k lower",
    icc_upper = "ICC3k upper") |>
  vline(j = c(1, 5, 9), part = "all") |>
  hline(i = 9) %>%
  font(fontname = "Times New Roman", part = "all") |>
  fontsize(part = "all", size = 12) |>
  set_table_properties(layout = "autofit")
```
























## Correlation Table

The correlation table of the model includes the dependent variable, controls as well as significant factors from the CFA. These are:

  *eecontlike
  *age
  *gender
  *pubfund
  *programpart
  *startupcenter
  *[CFA Factors]


```{r}

data |>
  select(eecontlike, age, gender, pubfund, programpart, startupcenter) |> #selecting variables
  apa.cor.table() #creation of correlation matrix

```

## Regression Model

The following regression modell represents a multilevel modell, as conjoint profiles were answered twice in two seperat rounds. 

We will include the earlier specified and listed variables.


### Model fitting

The linear regression model includes the following variables:

* Dependent variable: eecontlike = likelihood to engage in the EE contribution behavior
* Independent variables:
    **ee_support_reception = Perceived EE support
    **ee_affiliation = Affiliation to the EE
    **ee_relationships = Personal Relationship to EE actors
    **altruism_potential = Altruistic potential
    **positive_outcome_expectation = Expected positive outcome of the activity
    **benefit_perception = Perceived benefits of the entrepreneur
* Control variables:
    **age
    **gender
    **pubfund
    **programpart
    **startupcenter
    **CFA Factors
* Nesting variable: respondent_id

```{r}

res <- lmer(eecontlike ~ ee_support_reception + ee_affiliation + ee_relationships + altruism_potential + positive_outcome_expectation + benefit_perception + (1|respondent_id), REML = TRUE, data = data)

```


### Extracting Model Fit


```{r}

model_fit <- glance(res) # Extracting model fit

r2 <- r2_nakagawa(res, by_group = TRUE) # Computing R2

icc <- icc(res, by_group = TRUE) # Computing ICC 

# Creation of a fit table 
model_fit <- tibble(
  Variable = c(
    "R2 Level 1",
    "R2 Level 2",
    "ICC",
    "Sigma",
    "Decisions",
    "Respondents"),
      "B (β)" = c(
         round(r2$R2[1], 2),
         round(r2$R2[2], 2),
         round(icc$ICC[1],2),
        round(model_fit$sigma, 3),
        model_fit$nobs, length(unique(data$respondent_id))),
    SE = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, NA_real_),
    `t-ratio` = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, NA_real_),
     `p-value` = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, NA_real_)
)
```

### Standardize Coefficients

Subsequently we compute the standardized regression coefficients:

```{r}
# Obtaining standardized parameters
res_std <- standardize_parameters(res, method = "posthoc")

# Selecting standardized coefficients (rounded to 2 digits)

res_std <- res_std |>
  select(Std_Coefficient) |>
  mutate(Std_Coefficient = round(Std_Coefficient, 2))

```

### Cluster Robust Standard Errors

Subsequently we compute the cluster robust standard errors (SE) to allow for correlation between observations within clusters. This means we nest the 16 responses per respondent within respondent_ids.

Use the the "model_parameters" function ("parameters" package) to obtain cluster robust standard errors:

* vcov should be "vcovCR"
* vcov_args pick "CR1" for type and "dat$respondent_id" for cluster

```{r}
res <- model_parameters(res,
                        vcov = "vcovCR",
                        vcov_args = list(type = "CR1", cluster = data$respondent_id)
                        ) 

head(res)

```

## Prepring final output

```{r}
# Creating coefficient variable
res <- res |>
  mutate("B (β)" = paste0(B, " (", Std_Coefficient, ")")) |>
  select(-c(B, Std_Coefficient)) |>
  relocate("B (β)", .after = Variable)

# Combining result and fit tables
res <- rbind(res, model_fit)

# Renaming variables
res <- res |>
  mutate(Variable = if_else(Variable == "(Intercept)", "Intercept", Variable)) |>
  mutate(Variable = if_else(Variable == "ee_support_reception", "Perceived EE support", Variable)) |>
  mutate(Variable = if_else(Variable == "ee_affiliation", "Affiliation to the EE", Variable)) |>
  mutate(Variable = if_else(Variable == "ee_relationships", "Personal Relationship to EE actors", Variable)) |>
  mutate(Variable = if_else(Variable == "altruism_potential", "Altruistic potential", Variable)) |>
  mutate(Variable = if_else(Variable == "positive_outcome_expectation", "Expected positive outcome", Variable)) |>
  mutate(Variable = if_else(Variable == "benefit_perception", "Perceived benefits", Variable)) |>
  mutate(Variable = if_else(Variable == "age", "Age", Variable)) |>
  mutate(Variable = if_else(Variable == "gender", "Gender", Variable)) |>
  mutate(Variable = if_else(Variable == "pubfund", "Publicly funded", Variable)) |> 
  mutate(Variable = if_else(Variable == "programpart", "Program participation", Variable)) |>
  mutate(Variable = if_else(Variable == "startupcenter", "Startup center", Variable)) 
 
```

## Displaying Table

Use the flextable package to create a table with chained pipes:

* The "autofit()" function may come in handy
* Use "align" to right flush text of the 2nd column in the header
* Use "align" to right flush text of the 2nd column in the body
* Put a horizontal line below the footer "hline_bottom"
* Set the font to "Times New Roman" and fontsize to 12
* Output the table to word

```{r}

```
